{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPTfWLAhJm9dv5QhLJ03Gv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TahaErr/Civil_Rag/blob/main/Civil_Rag_Fixed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqRxZRrJ9_9O"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optimum"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cJtEvZT1Gm-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gptqmodel"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZGFp-1nwG4WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral 7B Modelinin Yüklenmesi (HuggingFace üzerinden)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2SGwTzQd-7Dq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "3Uw2CY1CAqdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import optimum\n",
        "\n",
        "model_id = \"TheBloke/Mistral-7B-Instruct-v0.1-GPTQ\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n"
      ],
      "metadata": {
        "id": "Mfgxe7kZ-yAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF\n"
      ],
      "metadata": {
        "id": "rgDrteGXBGde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "id": "fvMCE5jIBGgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "pdf_text = extract_text_from_pdf(\"/content/224R_01.pdf\")  # yüklediğin dosya adı\n",
        "print(pdf_text[:1000])  # ilk 1000 karakteri yazdır\n"
      ],
      "metadata": {
        "id": "Xu1WvmL6-yIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain sentence-transformers faiss-cpu\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xt66cvsA-yeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Metni LangChain belgesine dönüştür\n",
        "docs = [Document(page_content=pdf_text)]\n",
        "\n",
        "# Parçalama işlemi (300 karakterlik parçalar, 50 karakter üst üste binecek şekilde)\n",
        "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"{len(split_docs)} adet parça oluşturuldu.\")\n",
        "print(\"İlk parça:\\n\", split_docs[0].page_content)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "41AOOwolDDgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gD9wkK3uDRjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# HuggingFace tabanlı embedding modeli\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# FAISS veritabanını oluştur\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding_model)\n",
        "\n",
        "print(\"Embedding ve FAISS veritabanı başarıyla oluşturuldu.\")\n"
      ],
      "metadata": {
        "id": "7onqwR__DDkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "Ti7AG6XbEUuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example question in English\n",
        "user_question = \"What are the main causes of cracking in concrete?\"\n",
        "\n",
        "# Search relevant chunks from FAISS\n",
        "retrieved_docs = vectorstore.similarity_search(user_question, k=1)\n",
        "\n",
        "# Merge them into context\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n"
      ],
      "metadata": {
        "id": "CXlrQJeMD2Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are a civil engineer. Based on the following context, answer the question clearly and technically.\n",
        "\n",
        "\n",
        "Question:\n",
        "What are the main causes of cracking in concrete?\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "response = generator(prompt, max_new_tokens=300, do_sample=True, temperature=0.7, top_p=0.9)[0]['generated_text']\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "0VWIhXuCD2X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def generate_answer(question_text, max_tokens=300):\n",
        "    prompt = f\"\"\"\n",
        "You are a civil engineer. Based on the following context, answer the question clearly and technically.\n",
        "\n",
        "Question:\n",
        "{question_text}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )[0]['generated_text']\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "YHbW6a1wUxVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the main causes of cracking in concrete?\"\n",
        "print(generate_answer(question))\n"
      ],
      "metadata": {
        "id": "SfyI_o5nUxaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p9p2jemSUxdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# Hazır model pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Fonksiyonumuz\n",
        "def generate_answer(question_text, max_tokens=300):\n",
        "    prompt = f\"\"\"\n",
        "You are a civil engineer. Based on the following context, answer the question clearly and technically.\n",
        "\n",
        "Question:\n",
        "{question_text}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )[0]['generated_text']\n",
        "    return response\n",
        "\n",
        "# Gradio arayüzü\n",
        "demo = gr.Interface(\n",
        "    fn=generate_answer,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your technical question about concrete...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG-based Civil Engineering Assistant\",\n",
        "    description=\"Ask your question about concrete behavior, cracking, or construction — powered by Mistral 7B\"\n",
        ")\n",
        "\n",
        "# Başlat\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "Vn0iDz0gWBjm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}